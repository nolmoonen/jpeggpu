#include "decode_dc.hpp"
#include "decode_destuff.hpp"
#include "decode_huffman.hpp"
#include "decode_transpose.hpp"
#include "decoder_defs.hpp"
#include "defs.hpp"
#include "marker.hpp"
#include "reader.hpp"
#include "util.hpp"

#include <jpeggpu/jpeggpu.h>

#include <cub/block/block_scan.cuh>
#include <cub/device/device_reduce.cuh>
#include <cub/device/device_scan.cuh>
#include <cub/thread/thread_operators.cuh>

#include <cuda_runtime.h>

#include <cassert>
#include <type_traits>
#include <vector>

using namespace jpeggpu;

namespace {

/// \brief Contains all required information about the last synchronization point for the
///   subsequence. All information is relative to the segment.
struct subsequence_info {
    /// \brief Bit(!) position in scan. "Location of the last detected codeword."
    ///   TODO size_t?
    int p;
    /// \brief The number of decoded symbols.
    int n;
    /// \brief The data unit index in the MCU. Combined with the sampling factors, the color component
    ///   can be inferred. The paper calls this field "the current color component",
    ///   but merely checking the color component does not suffice.
    int c;
    /// \brief Zig-zag index.
    int z;
};

/// \brief Cached reader from global memory.
struct reader_state_global {
    __device__ reader_state_global(const uint8_t* scan) : scan(scan) {}

    // end_byte is index of first byte not included
    __device__ void reset(int byte_off, int begin_bit, int end_byte)
    {
        data     = scan + byte_off + begin_bit / 8;
        data_end = scan + end_byte;

        cache          = 0;
        cache_num_bits = 0;

        const int in_cache = (8 - (begin_bit % 8)) % 8;
        if (in_cache > 0) {
            cache          = *(data++);
            cache_num_bits = 8;
            discard_bits(8 - in_cache);
        }
    }

    /// \brief Moves data pointer forwards `num_bits` bits.
    __device__ void discard_bits(int num_bits)
    {
        assert(cache_num_bits >= num_bits);
        // set discarded bits to zero (upper bits in the cache)
        cache = cache & ((1 << (cache_num_bits - num_bits)) - 1);
        cache_num_bits -= num_bits;
    }

    /// \brief If there are enough bits in the input stream, loads `num_bits` into cache.
    /// \return Number of bits loaded.
    __device__ int load_bits(int num_bits)
    {
        while (cache_num_bits < num_bits) {
            if (data >= data_end) {
                break; // no more data to load
            }

            assert(data < data_end);
            assert(cache_num_bits + 8 < 32);

            // byte stuffing and restart markers are removed beforehand
            const uint8_t next_byte = *(data++);
            cache                   = (cache << 8) | next_byte;
            cache_num_bits += 8;
        }
        return min(cache_num_bits, num_bits);
    }

    /// \brief Peeks `num_bits` from cache, does not remove them.
    ///   Assumes enough bits are present.
    __device__ int select_bits(int num_bits)
    {
        assert(num_bits < 31);
        assert(cache_num_bits >= num_bits);

        // upper bits are zero
        return cache >> (cache_num_bits - num_bits);
    }

    const uint8_t* data;
    const uint8_t* data_end;
    int32_t cache; // new bits are at the least significant positions
    int cache_num_bits;

    const uint8_t* const scan;
};

template <int block_size>
struct reader_state_shared {
    using storage_t = uint32_t[chunk_size * block_size];

    __device__ reader_state_shared(
        storage_t& storage, const uint8_t* scan, const uint8_t* scan_end, int block_off_bytes)
        : storage(storage), scan(scan), block_off_bytes(block_off_bytes)
    {
        assert(block_off_bytes % 4 == 0); // alignment requirement
        const uint32_t* scan_block = reinterpret_cast<const uint32_t*>(scan + block_off_bytes);
        for (int i = 0; i < chunk_size; ++i) {
            const int idx = i * block_size + threadIdx.x;
            // TODO do no load OOB values
            // storage[idx]  = scan_block[idx];
            // FIXME misaligned address? temp fix
            for (int j = 0; j < 4; ++j) {
                if (block_off_bytes + idx * 4 + j >= scan_end - scan) continue;
                reinterpret_cast<uint8_t*>(&storage[idx])[j] =
                    reinterpret_cast<const uint8_t*>(&scan_block[idx])[j];
            }
        }
        __syncthreads();
    }

    // end_byte is index of first byte not included
    __device__ void reset(int byte_off, int begin_bit, int end_byte)
    {
        assert(byte_off >= 0);
        assert(begin_bit >= 0);
        const int total_byte_off = byte_off + begin_bit / 8;
        if (!(total_byte_off >= block_off_bytes)) {
            printf(
                "%d %d %d %d %d\n", byte_off, begin_bit, block_off_bytes, threadIdx.x, blockIdx.x);
        }
        assert(total_byte_off >= block_off_bytes);

        idx_in_shared = total_byte_off - block_off_bytes;
        bit_off       = (8 - (begin_bit % 8)) % 8;
    }

    /// \brief Moves data pointer forwards `num_bits` bits.
    __device__ void discard_bits(int num_bits)
    {
        bit_off += num_bits;
        if (bit_off >= 32) {
            bit_off -= 32;
            ++idx_in_shared;
        }
    }

    /// \brief If there are enough bits in the input stream, loads `num_bits` into cache.
    /// \return Number of bits loaded.
    __device__ int load_bits(int num_bits)
    {
        const int remaining = (chunk_size * block_size - 1 - idx_in_shared) * 32 - (32 - bit_off);
        return min(num_bits, remaining);
    }

    /// \brief Peeks `num_bits` from cache, does not remove them.
    ///   Assumes enough bits are present.
    __device__ int select_bits(int num_bits)
    {
        assert(0 <= idx_in_shared && idx_in_shared + 1 < chunk_size * block_size);
        uint64_t ret = (uint64_t{storage[idx_in_shared]} << 32) | storage[idx_in_shared + 1];
        ret <<= bit_off;
        ret >>= 64 - num_bits;
        return ret;
    }

    int idx_in_shared;
    int bit_off;

    storage_t& storage;
    const uint8_t* const scan;
    const int block_off_bytes;
};

/// \brief Get the Huffman category from stream.
///
/// \tparam do_discard Whether to discard the bits that were read in the process.
/// \param[out] length Number of bits read.
template <bool do_discard = true, typename reader_state>
uint8_t __device__ get_category(reader_state& rstate, int& length, const huffman_table& table)
{
    // due to possibly guessing the huffman table wrong, there may not be enough bits left
    const int max_bits = rstate.load_bits(16);
    if (max_bits == 0) {
        // exit if there are no bits
        length = 0;
        return 0;
    }
    int i;
    int32_t code;
    for (i = 0; i < max_bits; ++i) {
        code                    = rstate.select_bits(i + 1);
        const bool is_last_iter = i == (max_bits - 1);
        if (code <= table.maxcode[i] || is_last_iter) {
            break;
        }
    }
    assert(1 <= i + 1 && i + 1 <= 16);
    // termination condition: 1 <= i + 1 <= 16, i + 1 is number of bits
    if constexpr (do_discard) {
        rstate.discard_bits(i + 1);
    }
    length        = i + 1;
    const int idx = table.valptr[i] + (code - table.mincode[i]);
    if (idx < 0 || 256 <= idx) {
        // found a value that does not make sense. this can happen if the wrong huffman
        //   table is used. return arbitrary value
        return 0;
    }
    return table.huffval[idx];
}

__device__ int get_value(int num_bits, int code)
{
    // TODO leftshift negative value is UB
    return code < ((1 << num_bits) >> 1) ? (code + ((-1) << num_bits) + 1) : code;
}

template <typename reader_state>
__device__ void decode_next_symbol_dc(
    reader_state& rstate,
    int& length,
    int& symbol,
    int& run_length,
    const huffman_table& table_dc,
    const huffman_table& table_ac,
    int z)
{
    int category_length    = 0;
    const uint8_t category = get_category(rstate, category_length, table_dc);

    if (category != 0) {
        assert(0 < category && category < 17);
        const int loaded_bits = rstate.load_bits(category);
        // there might not be `category` bits left
        if (loaded_bits < category) {
            // eat all remaining so the `decode_subsequence` loop does not get stuck
            length = category_length + loaded_bits;
            rstate.discard_bits(loaded_bits);
            symbol     = 0; // arbitrary symbol
            run_length = 0; // arbitrary length
            return;
        }
        const int offset = rstate.select_bits(category);
        rstate.discard_bits(category);
        const int value = get_value(category, offset);

        length = category_length + category;
        symbol = value;
    } else {
        length = category_length;
        symbol = 0;
    }

    // peek next to determine run (is always AC)
    {
        int len;
        const uint8_t s    = get_category<false>(rstate, len, table_ac);
        const int run      = s >> 4;
        const int category = s & 0xf;

        if (category != 0) {
            run_length = run;
        } else {
            // either EOB or ZRL, which are treated as a symbol,
            //   so there are no zeros inbetween the DC value and EOB or ZRL
            run_length = 0;
        }
    }
};

template <typename reader_state>
__device__ void decode_next_symbol_ac(
    reader_state& rstate,
    int& length,
    int& symbol,
    int& run_length,
    const huffman_table& table,
    int z)
{
    int category_length = 0;
    const uint8_t s     = get_category(rstate, category_length, table);
    const int run       = s >> 4;
    const int category  = s & 0xf;

    if (category != 0) {
        const int loaded_bits = rstate.load_bits(category);
        // there might not be `category` bits left
        if (loaded_bits < category) {
            // eat all remaining so the `decode_subsequence` loop does not get stuck
            length = category_length + loaded_bits;
            rstate.discard_bits(loaded_bits);
            symbol     = 0; // arbitrary symbol
            run_length = 0; // arbitrary length
            return;
        }
        const int offset = rstate.select_bits(category);
        rstate.discard_bits(category);
        const int value = get_value(category, offset);

        length = category_length + category;
        symbol = value;

        if (z + 1 <= 63) { // note: z already includes `run`
            // next value is ac coefficient, peek next to determine run
            int len;
            const uint8_t s    = get_category<false>(rstate, len, table);
            const int run      = s >> 4;
            const int category = s & 0xf;

            if (category != 0) {
                run_length = run;
            } else {
                // EOB or ZRL
                run_length = 0;
            }
        } else {
            // next table is dc
            run_length = 0;
        }
    } else {
        if (run == 15) {
            length     = category_length;
            symbol     = 0; // ZRL
            run_length = 15;

            if (z + 1 + 15 <= 63) {
                // there is an AC symbol after the ZRL
                int len;
                const uint8_t s    = get_category<false>(rstate, len, table);
                const int run      = s >> 4;
                const int category = s & 0xf;

                if (category != 0) {
                    run_length += run;
                } else {
                    // EOB or ZRL
                }
            } else {
                // next is dc
            }
        } else {
            length     = category_length;
            symbol     = 0; // EOB
            run_length = 63 - z;
        }
    }
};

/// \brief Extracts coefficients from the bitstream while switching between DC and AC Huffman
/// tables.
///
/// - If symbol equals ZRL, 15 will be returned for run_length.
/// - If symbol equals EOB, 63 - z will be returned for run_length, with z begin the current
///     index in the zig-zag sequence.
///
/// \param[inout] rstate
/// \param[out] length The number of processed bits. Will be non-zero.
/// \param[out] symbol The decoded coefficient, provided the code was not EOB or ZRL.
/// \param[out] run_length The run-length of zeroes which the coefficient is followed by.
/// \param[in] table
/// \param[in] z Current index in the zig-zag sequence.
template <typename reader_state>
__device__ void decode_next_symbol(
    reader_state& rstate,
    int& length,
    int& symbol,
    int& run_length,
    const huffman_table& table_dc,
    const huffman_table& table_ac,
    int z)
{
    if (z == 0) {
        decode_next_symbol_dc(rstate, length, symbol, run_length, table_dc, table_ac, z);
    } else {
        decode_next_symbol_ac(rstate, length, symbol, run_length, table_ac, z);
    }
}

enum class component {
    y, // Y (YCbCR) or C (CMYK)
    cb, // Cb (YCbCR) or M (CMYK)
    cr, // Cr (YCbCR) or Y (CMYK)
    k // k (CMYK)
};

struct const_state {
    const uint8_t* scan;
    const uint8_t* scan_end;
    const huffman_table* dc_0;
    const huffman_table* ac_0;
    const huffman_table* dc_1;
    const huffman_table* ac_1;
    const huffman_table* dc_2;
    const huffman_table* ac_2;
    const huffman_table* dc_3;
    const huffman_table* ac_3;
    int2 ss_0;
    int2 ss_1;
    int2 ss_2;
    int2 ss_3;
    int num_data_units_in_mcu;
    int num_components;
    int num_data_units;
    int num_mcus_in_segment;
};

/// \brief Infer image components based on data unit index `c` (in MCU).
__device__ component calc_component(const const_state& cstate, int c)
{
    const int num_in_0 = cstate.ss_0.x * cstate.ss_0.y;
    if (c < num_in_0) {
        return component::y;
    }
    c -= num_in_0;
    const int num_in_1 = cstate.ss_1.x * cstate.ss_1.y;
    if (c < num_in_1) {
        return component::cb;
    }
    c -= num_in_1;
    const int num_in_2 = cstate.ss_2.x * cstate.ss_2.y;
    if (c < num_in_2) {
        return component::cr;
    }
    c -= num_in_2;
    const int num_in_3 = cstate.ss_3.x * cstate.ss_3.y;
    if (c < num_in_3) {
        return component::k;
    }
    assert(false);
    return component::k;
}

static_assert(std::is_trivially_copyable_v<const_state>);

/// \brief Algorithm 2.
///
/// \tparam is_overflow Whether `i` was decoded by another thread already. TODO word this better.
/// \tparam do_write Whether to write the coefficients to the output buffer.
/// \param i Global subsequence index.
/// \param segment_info Segment info for the segment that subsequence `i` is in.
/// \param segment_idx The index of the segment subsequence `i` is in.
template <bool is_overflow, bool do_write, typename reader_state>
__device__ subsequence_info decode_subsequence(
    int i,
    int16_t* out,
    subsequence_info* s_info,
    const const_state& cstate,
    const segment_info& segment_info,
    int segment_idx,
    reader_state& rstate)
{
    assert(i >= segment_info.subseq_offset);
    const int i_rel = i - segment_info.subseq_offset;

    subsequence_info info;
    int position_in_output = 0;
    if constexpr (do_write) {
        // offset in pixels
        int segment_offset = segment_idx * cstate.num_mcus_in_segment *
                             cstate.num_data_units_in_mcu * data_unit_size;
        // subsequence_info.n is relative to segment
        position_in_output = segment_offset + s_info[i].n;
    }

    if constexpr (is_overflow) {
        assert(i > 0);
        info.p = s_info[i - 1].p;
        assert(info.p >= (i_rel - 1) * subsequence_size);
        // do not load `n` here, to achieve that `s_info.n` is the number of decoded symbols
        //   only for each subsequence (and not an aggregate)
        info.n = 0;
        info.c = s_info[i - 1].c;
        info.z = s_info[i - 1].z;

        assert(segment_info.begin >= 0);
        rstate.reset(segment_info.begin, info.p, segment_info.end);
    } else {
        // start of i-th subsequence
        info.p = i_rel * subsequence_size;
        info.n = 0;
        info.c = 0; // start from the first data unit of the Y component
        info.z = 0;

        // TODO info.p is always multiple of 8
        rstate.reset(segment_info.begin, info.p, segment_info.end);
    }

    const int end_subseq  = (i_rel + 1) * subsequence_size; // first bit in next subsequence
    const int end_segment = (segment_info.end - segment_info.begin) * 8; // bit count in segment
    subsequence_info last_symbol{-1, -1, -1, -1}; // the last detected codeword
    assert(info.p < min(end_subseq, end_segment));
    while (info.p < min(end_subseq, end_segment)) {
        // check if we have all blocks. this is needed since the scan is padded to a 8-bit multiple
        //   (so info.p cannot reliably be used to determine if the loop should break)
        //   this problem is excerbated by restart intevals, where padding occurs more frequently
        if (do_write && position_in_output >= (segment_idx + 1) * cstate.num_mcus_in_segment *
                                                  cstate.num_data_units_in_mcu * data_unit_size) {
            break;
        }

        last_symbol = info;

        const component comp    = calc_component(cstate, info.c);
        int length              = 0;
        int symbol              = 0;
        int run_length          = 0;
        const huffman_table* dc = nullptr;
        const huffman_table* ac = nullptr;
        switch (comp) {
        case component::y:
            dc = cstate.dc_0;
            ac = cstate.ac_0;
            break;
        case component::cb:
            dc = cstate.dc_1;
            ac = cstate.ac_1;
            break;
        case component::cr:
            dc = cstate.dc_2;
            ac = cstate.ac_2;
            break;
        case component::k:
            dc = cstate.dc_3;
            ac = cstate.ac_3;
            break;
        }
        // always returns length > 0 if there are bits in `rstate` to ensure progress
        decode_next_symbol(rstate, length, symbol, run_length, *dc, *ac, info.z);
        if (do_write) {
            // TODO could make a separate kernel for this
            const int data_unit_idx    = position_in_output / data_unit_size;
            const int idx_in_data_unit = position_in_output % data_unit_size;
            out[data_unit_idx * data_unit_size + order_natural[idx_in_data_unit]] = symbol;
        }
        if (do_write) {
            position_in_output += run_length + 1;
        }
        info.p += length;
        info.n += run_length + 1;
        info.z += run_length + 1;

        if (info.z >= 64) {
            // the data unit is complete
            info.z = 0;
            ++info.c;

            if (info.c >= cstate.num_data_units_in_mcu) {
                // mcu is complete
                info.c = 0;
            }
        }
    }

    return last_symbol;
}

/// \brief Decode all subsequences once without synchronizing.
__global__ void decode_subsequences(
    subsequence_info* s_info,
    int num_subsequences,
    const_state cstate,
    const segment_info* segment_infos,
    const int* segment_indices)
{
    const int subseq_idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (subseq_idx >= num_subsequences) {
        return;
    }

    // obtain the segment info for this subsequence
    const int segment_idx       = segment_indices[subseq_idx];
    const segment_info seg_info = segment_infos[segment_idx];
    assert(seg_info.begin >= 0);

    reader_state_global rstate(cstate.scan);

    // FIXME: realization: some bytes are "lost" due to rounding up to subsequences.
    //   due to this, the ith subsequence may not start at i * (#bytes in subseq)
    // FIXME: is it possible to round up so that this is the case? it'll make offset calculation easier as well
    assert(subseq_idx * subsequence_size >= blockDim.x * blockIdx.x * subsequence_size_bytes);
    assert(seg_info.subseq_offset <= subseq_idx);
    assert(
        seg_info.begin + (subseq_idx - seg_info.subseq_offset) * subsequence_size >=
        blockDim.x * blockIdx.x * subsequence_size_bytes);
    subsequence_info info = decode_subsequence<false, false>(
        subseq_idx, nullptr, s_info, cstate, seg_info, segment_idx, rstate);
    assert(seg_info.subseq_offset <= subseq_idx);
    assert(info.p >= (subseq_idx - seg_info.subseq_offset) * subsequence_size);
    if (!(seg_info.begin + info.p / 8 >= blockDim.x * blockIdx.x * subsequence_size_bytes)) {
        printf(
            "%d %d %d %d\n",
            seg_info.begin,
            info.p / 8,
            seg_info.begin + info.p / 8,
            blockDim.x * blockIdx.x * subsequence_size_bytes);
    }
    assert(seg_info.begin + info.p / 8 >= blockDim.x * blockIdx.x * subsequence_size_bytes);
    s_info[subseq_idx].p = info.p;
    // paper text does not mention `n` should be stored here, but if not storing `n`
    //   the first subsequence info's `n` will not be initialized. for simplicity, store all
    s_info[subseq_idx].n = info.n;
    s_info[subseq_idx].c = info.c;
    s_info[subseq_idx].z = info.z;
}

struct logical_and {
    __device__ bool operator()(const bool& lhs, const bool& rhs) { return lhs && rhs; }
};

/// \brief Synchronize between sequences. Each thread handles one sequence,
///   the last sequence requires no handling. Meaning is changed w.r.t. paper!

/// \brief Intra sequence synchronization (alg-3:05-23).
///   Each thread handles one subsequence at a time. Starting from each unique subsequence,
///   decode one subsequence at a time until the result is equal to the result of a different
///   thread having decoded that subsequence. If that is the case, the result is correct and
///   this thread is done.
///
/// \tparam block_size "b", the number of adjacent subsequences that form a sequence,
///   equal to the block size of this kernel.

/// \brief Synchronizes subsequences in multiple contiguous arrays of subsequences.
///   Each thread handles such a contiguous array.
///
///   W.r.t. the paper, `size_in_subsequences` of 1 and `block_size` equal to "b" will be
///     equivalent to "intersequence synchronization".
///
///
///
/// \tparam size_in_subsequences Amount of contigous subsequences are synchronized.
/// \tparam block_size Block size of the kernel.
template <int size_in_subsequences, int block_size>
__global__ void sync_subsequences(
    subsequence_info* s_info,
    int num_subsequences,
    const_state cstate,
    const segment_info* segment_infos,
    const int* segment_indices)
{
    assert(blockDim.x == block_size);
    const int tid             = blockDim.x * blockIdx.x + threadIdx.x;
    // subsequence index this thread is overflowing from, this will be the first s_info read
    const int subseq_idx_from = (tid + 1) * size_in_subsequences - 1;
    // overflowing to
    const int subseq_idx_to   = subseq_idx_from + 1;

    // segment index from the subsequence we are flowing from
    const int segment_idx       = segment_indices[subseq_idx_from];
    const segment_info seg_info = segment_infos[segment_idx];
    const int bytes_in_segment  = seg_info.end - seg_info.begin;
    // index of the final subsequence for this segment
    const int subseq_last_idx_segment =
        seg_info.subseq_offset +
        ceiling_div(bytes_in_segment, static_cast<unsigned int>(subsequence_size_bytes)) - 1;

    // first subsequence index owned/read by the next thread block, follow from `subseq_idx_from`
    //   calculation, substituting `blockDim.x` by `blockDim.x + 1` and `threadIdx.x` by `0`
    const int subseq_idx_from_next_block =
        ((blockIdx.x + 1) * blockDim.x + 1) * size_in_subsequences - 1;
    // last subsequence index "owned" by this thread block
    const int subseq_last_idx_block = subseq_idx_from_next_block - 1;

    // last index dictated by block, segment, and JPEG stream
    const int end = min(subseq_last_idx_segment, min(subseq_last_idx_block, num_subsequences - 1));

    // if index is within bounds, segment final index must be correctly calculated
    assert(subseq_idx_from > end || subseq_idx_from <= subseq_last_idx_segment);

    __shared__ bool is_block_done;
    using block_reduce = cub::BlockReduce<bool, block_size>;
    __shared__ typename block_reduce::TempStorage temp_storage;

    reader_state_global rstate(cstate.scan);

    bool is_synced = false;
    bool do_write  = true;
    for (int i = 0; i < blockDim.x * size_in_subsequences; ++i) {
        const int subseq_idx = subseq_idx_to + i;
        subsequence_info info;
        if (subseq_idx <= end && !is_synced) {
            info = decode_subsequence<true, false>(
                subseq_idx, nullptr, s_info, cstate, seg_info, segment_idx, rstate);
            const subsequence_info& stored_info = s_info[subseq_idx];
            if (info.p == stored_info.p && info.c == stored_info.c && info.z == stored_info.z) {
                // synchronization is achieved: the decoding process of this thread has found
                //   the same "outcome" for the `subseq_idx`th subsequence as the stored result
                //   of a decoding process that started from an earlier point in the JPEG stream,
                //   meaning that this outcome is correct.
                is_synced = true;
            }
        } else {
            do_write = false;
        }
        bool is_thread_done      = is_synced || subseq_idx > end;
        bool is_block_done_local = block_reduce(temp_storage).Reduce(is_thread_done, logical_and{});
        __syncthreads(); // await s_info reads
        if (threadIdx.x == 0) is_block_done = is_block_done_local;
        if (do_write) s_info[subseq_idx] = info;
        __syncthreads(); // await s_info writes and is_block_done write
        if (is_block_done) return;
    }
}

template <int block_size>
__global__ void sync_subsequences_opt(
    subsequence_info* s_info,
    int num_subsequences,
    const_state cstate,
    const segment_info* segment_infos,
    const int* segment_indices)
{
    assert(blockDim.x == block_size);
    const int tid = blockDim.x * blockIdx.x + threadIdx.x;

    const int segment_idx       = segment_indices[tid];
    const segment_info seg_info = segment_infos[segment_idx];
    assert(seg_info.begin >= 0);
    const int bytes_in_segment = seg_info.end - seg_info.begin;
    assert(bytes_in_segment > 0);
    const int subseq_last_idx_segment =
        seg_info.subseq_offset +
        ceiling_div(bytes_in_segment, static_cast<unsigned int>(subsequence_size_bytes)) - 1;

    const int subseq_idx_from_next_block = (blockIdx.x + 1) * blockDim.x;
    const int subseq_last_idx_block      = subseq_idx_from_next_block - 1;

    const int end = min(subseq_last_idx_segment, min(subseq_last_idx_block, num_subsequences - 1));

    assert(tid + 1 > end || tid + 1 <= subseq_last_idx_segment);

    __shared__ bool is_block_done;
    using block_reduce = cub::BlockReduce<bool, block_size>;
    __shared__ typename block_reduce::TempStorage temp_storage;

    // FIXME the use of this struct needs some explanation that we (may be?) are explicitly
    //   forcing a desync between thread blocks because we are not reading symbols until we
    //   have read more than the subsequence contains (like the paper does)
    using reader_state_t = reader_state_shared<block_size>;
    __shared__ typename reader_state_t::storage_t storage;
    const int block_off = blockDim.x * blockIdx.x * subsequence_size_bytes;
    reader_state_t rstate(storage, cstate.scan, cstate.scan_end, block_off);

    bool is_synced = false;
    bool do_write  = true;
    for (int i = 0; i < block_size; ++i) {
        const int subseq_idx = tid + 1 + i;
        subsequence_info info;
        if (subseq_idx <= end && !is_synced) {
            info = decode_subsequence<true, false>(
                subseq_idx, nullptr, s_info, cstate, seg_info, segment_idx, rstate);
            const subsequence_info& stored_info = s_info[subseq_idx];
            if (info.p == stored_info.p && info.c == stored_info.c && info.z == stored_info.z) {
                is_synced = true;
            }
        } else {
            do_write = false;
        }
        bool is_thread_done      = is_synced || subseq_idx > end;
        bool is_block_done_local = block_reduce(temp_storage).Reduce(is_thread_done, logical_and{});
        __syncthreads();
        if (threadIdx.x == 0) is_block_done = is_block_done_local;
        if (do_write) s_info[subseq_idx] = info;
        __syncthreads();
        if (is_block_done) return;
    }
}

__global__ void decode_write(
    int16_t* out,
    subsequence_info* s_info,
    int num_subsequences,
    const_state cstate,
    const segment_info* segment_infos,
    const int* segment_indices)
{
    const int si = blockIdx.x * blockDim.x + threadIdx.x;
    if (si >= num_subsequences) {
        return;
    }

    const int segment_idx       = segment_indices[si];
    const segment_info seg_info = segment_infos[segment_idx];

    reader_state_global rstate(cstate.scan);

    // only first thread does not do overflow
    constexpr bool do_write = true;
    if (si == seg_info.subseq_offset) {
        decode_subsequence<false, do_write>(si, out, s_info, cstate, seg_info, segment_idx, rstate);
    } else {
        decode_subsequence<true, do_write>(si, out, s_info, cstate, seg_info, segment_idx, rstate);
    }
}

struct sum_subsequence_info {
    __device__ __forceinline__ subsequence_info
    operator()(const subsequence_info& a, const subsequence_info& b) const
    {
        // asserts in the comparison function are not great since CUB may execute the comparator on
        // garbage data if the block or warp is not completely full
        return {0, a.n + b.n, 0, 0};
    }
};

/// \brief Copy `subsequence_info::n` from `src` to `dst`.
__global__ void assign_sinfo_n(
    int num_subsequences, subsequence_info* dst, const subsequence_info* src)
{
    const int lid = blockDim.x * blockIdx.x + threadIdx.x;
    if (lid >= num_subsequences) {
        return;
    }

    assert(src[lid].n >= 0);
    dst[lid].n = src[lid].n;
}

} // namespace

template <bool do_it>
jpeggpu_status jpeggpu::decode_scan(
    const jpeg_stream& info,
    const uint8_t* d_scan_destuffed,
    const segment_info* d_segment_infos,
    const int* d_segment_indices,
    int16_t* d_out,
    const struct scan& scan,
    huffman_table* (&d_huff_tables)[max_huffman_count][HUFF_COUNT],
    stack_allocator& allocator,
    cudaStream_t stream)
{
    // "N": number of subsequences, determined by JPEG stream
    const int num_subsequences = scan.num_subsequences;

    // alg-1:01
    int num_data_units = 0;
    for (int c = 0; c < info.num_components; ++c) {
        num_data_units += (info.components[c].data_size_x / jpeggpu::data_unit_vector_size) *
                          (info.components[c].data_size_y / jpeggpu::data_unit_vector_size);
    }

    // alg-1:05
    subsequence_info* d_s_info;
    JPEGGPU_CHECK_STAT(
        allocator.reserve<do_it>(&d_s_info, num_subsequences * sizeof(subsequence_info)));

    const const_state cstate = {
        d_scan_destuffed,
        // this is not the end of the destuffed data, but the end of the stuffed allocation.
        //   the final subsequence may read garbage bits (but not bytes).
        //   this can introduce additional (non-existent) symbols,
        //   but a check is in place to prevent writing more symbols than needed
        d_scan_destuffed + (scan.end - scan.begin),
        d_huff_tables[info.components[0].dc_idx][HUFF_DC],
        d_huff_tables[info.components[0].ac_idx][HUFF_AC],
        d_huff_tables[info.components[1].dc_idx][HUFF_DC],
        d_huff_tables[info.components[1].ac_idx][HUFF_AC],
        d_huff_tables[info.components[2].dc_idx][HUFF_DC],
        d_huff_tables[info.components[2].ac_idx][HUFF_AC],
        d_huff_tables[info.components[3].dc_idx][HUFF_DC],
        d_huff_tables[info.components[3].ac_idx][HUFF_AC],
        make_int2(info.components[0].ss_x, info.components[0].ss_y),
        make_int2(info.components[1].ss_x, info.components[1].ss_y),
        make_int2(info.components[2].ss_x, info.components[2].ss_y),
        make_int2(info.components[3].ss_x, info.components[3].ss_y),
        info.num_data_units_in_mcu,
        info.num_components,
        num_data_units,
        info.restart_interval != 0 ? info.restart_interval : info.num_mcus_x * info.num_mcus_y};

    // decode all subsequences
    // "b", sequence size in number of subsequences, configurable
    constexpr int num_subsequences_in_sequence = 256;
    // "B", number of sequences
    const int num_sequences =
        ceiling_div(num_subsequences, static_cast<unsigned int>(num_subsequences_in_sequence));
    if (do_it) {
        log("decoding %d subsequences\n", num_subsequences);
        decode_subsequences<<<num_sequences, num_subsequences_in_sequence, 0, stream>>>(
            d_s_info, num_subsequences, cstate, d_segment_infos, d_segment_indices);
        JPEGGPU_CHECK_CUDA(cudaGetLastError());
    }
    JPEGGPU_CHECK_CUDA(cudaStreamSynchronize(stream)); // FIXME remove

    // synchronize intra sequence/inter subsequence
    if (do_it && num_subsequences > 1) {
        log("intra sync of %d blocks of %d subsequences\n",
            num_sequences,
            num_subsequences_in_sequence);
        sync_subsequences_opt<num_subsequences_in_sequence>
            <<<num_sequences, num_subsequences_in_sequence, 0, stream>>>(
                d_s_info, num_subsequences, cstate, d_segment_infos, d_segment_indices);
        JPEGGPU_CHECK_CUDA(cudaGetLastError());
    }
    JPEGGPU_CHECK_CUDA(cudaStreamSynchronize(stream)); // FIXME remove

    // synchronize intra supersequence/inter sequence
    constexpr int num_sequences_in_supersequence = 512; // configurable
    const int num_supersequences =
        ceiling_div(num_sequences, static_cast<unsigned int>(num_sequences_in_supersequence));
    if (do_it && num_sequences > 1) {
        log("intra sync of %d blocks of %d subsequences\n",
            num_supersequences,
            num_sequences_in_supersequence * num_subsequences_in_sequence);
        sync_subsequences<num_subsequences_in_sequence, num_sequences_in_supersequence>
            <<<num_supersequences, num_sequences_in_supersequence, 0, stream>>>(
                d_s_info, num_subsequences, cstate, d_segment_infos, d_segment_indices);
        JPEGGPU_CHECK_CUDA(cudaGetLastError());
    }

    if (num_supersequences > num_sequences_in_supersequence) {
        constexpr int max_byte_size =
            subsequence_size_bytes * num_subsequences_in_sequence * num_sequences_in_supersequence;
        log("byte stream is larger than max supported (%d bytes)\n", max_byte_size);
        return JPEGGPU_NOT_SUPPORTED;
    }

    // TODO consider SoA or do in-place
    // alg-1:07-08
    subsequence_info* d_reduce_out;
    JPEGGPU_CHECK_STAT(
        allocator.reserve<do_it>(&d_reduce_out, num_subsequences * sizeof(subsequence_info)));
    if (do_it) {
        // TODO debug to satisfy initcheck
        JPEGGPU_CHECK_CUDA(
            cudaMemsetAsync(d_reduce_out, 0, num_subsequences * sizeof(subsequence_info), stream));
    }

    const subsequence_info init_value{0, 0, 0, 0};
    void* d_tmp_storage      = nullptr;
    size_t tmp_storage_bytes = 0;
    JPEGGPU_CHECK_CUDA(cub::DeviceScan::ExclusiveScanByKey(
        d_tmp_storage,
        tmp_storage_bytes,
        d_segment_indices, // d_keys_in
        d_s_info,
        d_reduce_out,
        sum_subsequence_info{},
        init_value,
        num_subsequences,
        cub::Equality{},
        stream));

    JPEGGPU_CHECK_STAT(allocator.reserve<do_it>(&d_tmp_storage, tmp_storage_bytes));
    if (do_it) {
        // TODO debug to satisfy initcheck
        JPEGGPU_CHECK_CUDA(cudaMemsetAsync(d_tmp_storage, 0, tmp_storage_bytes, stream));
    }

    if (do_it) {
        JPEGGPU_CHECK_CUDA(cub::DeviceScan::ExclusiveScanByKey(
            d_tmp_storage,
            tmp_storage_bytes,
            d_segment_indices, // d_keys_in
            d_s_info,
            d_reduce_out,
            sum_subsequence_info{},
            init_value,
            num_subsequences,
            cub::Equality{},
            stream));
    }

    constexpr int block_size_assign = 256;
    const int grid_dim =
        ceiling_div(num_subsequences, static_cast<unsigned int>(block_size_assign));
    if (do_it) {
        assign_sinfo_n<<<grid_dim, block_size_assign, 0, stream>>>(
            num_subsequences, d_s_info, d_reduce_out);
        JPEGGPU_CHECK_CUDA(cudaGetLastError());
    }

    if (do_it) {
        // alg-1:09-15
        decode_write<<<num_sequences, num_subsequences_in_sequence, 0, stream>>>(
            d_out, d_s_info, num_subsequences, cstate, d_segment_infos, d_segment_indices);
        JPEGGPU_CHECK_CUDA(cudaGetLastError());
    }

    return JPEGGPU_SUCCESS;
}

template jpeggpu_status jpeggpu::decode_scan<false>(
    const jpeg_stream&,
    const uint8_t*,
    const segment_info*,
    const int*,
    int16_t*,
    const struct scan&,
    huffman_table* (&)[max_huffman_count][HUFF_COUNT],
    stack_allocator&,
    cudaStream_t);

template jpeggpu_status jpeggpu::decode_scan<true>(
    const jpeg_stream&,
    const uint8_t*,
    const segment_info*,
    const int*,
    int16_t*,
    const struct scan&,
    huffman_table* (&)[max_huffman_count][HUFF_COUNT],
    stack_allocator&,
    cudaStream_t);
